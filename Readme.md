# ğŸš€ Lunar Landing Program Using Reinforcement Learning

An AI-powered simulation that trains an intelligent agent to **autonomously land a spacecraft using Deep Reinforcement Learning (DRL)**.
The agent learns optimal landing strategies using the **Deep Q-Network (DQN)** algorithm by interacting with the **OpenAI Gym LunarLander environment**.

This project demonstrates how reinforcement learning enables machines to **learn complex control tasks through trial and error**.

# Project Demonstration
- ## Before Training
<p align="center">
  <img src="Images\before_training.webp" alt="Before Training" width="100%">
</p>

- ## After Training
<p align="center">
  <img src="Images\After_training.webp" alt="After Training" width="100%">
</p>

---

# ğŸ“Œ Project Overview

Landing a spacecraft safely requires controlling velocity, direction, and position simultaneously.

In this project:

* An **RL agent interacts with the LunarLander environment**
* The agent receives **rewards and penalties**
* A **Deep Q-Network** learns the best actions
* Over multiple episodes, the agent learns how to land safely.

The goal is to **maximize cumulative rewards while achieving stable landing behavior**.

---

# ğŸ§  Key Reinforcement Learning Concepts

### Deep Q Network (DQN)

A neural network approximates the **Q-value function**, which estimates the expected reward for each action in a given state.

### Experience Replay

Past experiences are stored in memory and randomly sampled during training to improve learning stability.

Experience tuple:

```text
(state, action, reward, next_state, done)

```

### Target Network

A separate network used to calculate stable target Q-values during training.

### Exploration vs Exploitation

The agent follows an **epsilon-greedy strategy**:

* Exploration â†’ Random actions
* Exploitation â†’ Best predicted action

Epsilon gradually decreases during training.

### Bellman Equation

Q-values are updated using:

```text
Q(s,a) = r + Î³ max Q(s',a')

```

Where

* r = reward
* Î³ = discount factor
* s' = next state

---

# ğŸ—ï¸ System Architecture

```mermaid
flowchart TD
    START([" Training Starts"]):::startend

    RESET["**Reset Environment**<br/>Get initial state sâ‚€"]:::process

    EPISODE_LOOP{{"Episode < Max Episodes?"}}:::decision

    STEP_LOOP{{"Step < Max Steps<br/>& Not Done?"}}:::decision

    AGENT["**Agent**<br/>Îµ-Greedy Policy<br/>Select action aâ‚œ"]:::process

    ENV["**Environment**<br/>LunarLander-v3<br/>Execute action aâ‚œ<br/>Return sâ‚œâ‚Šâ‚, râ‚œ, done"]:::env

    STORE["**Store Transition**<br/>Add âŸ¨sâ‚œ, aâ‚œ, râ‚œ, sâ‚œâ‚Šâ‚, doneâŸ©<br/>to Replay Memory"]:::database

    UPDATE_CHECK{{"Time to Update?<br/>(Every 4 steps)"}}:::decision

    SAMPLE["**Sample Mini-batch**<br/>From Replay Memory"]:::process

    ONLINE["** Q-Network**<br/>Compute Q(s,a) for batch"]:::mlmodel

    TARGET["**Target Q-Network**<br/>Compute TD targets<br/>y = r + Î³Â·max Q'(s',a')"]:::mlmodel

    BELLMAN["**Bellman Update**<br/>Loss = MSE(Q - y)Â²<br/>Backprop Â· Update Î¸"]:::process

    SYNC_CHECK{{"Time to Sync?<br/>(Every 4 steps)"}}:::decision

    SYNC["**Sync Target Network**<br/>Î¸' â† Î¸"]:::process

    EPSILON["**Update Epsilon**<br/>Îµ â† max(Îµ_min, ÎµÂ·decay)"]:::process

    REWARD_CHECK{{"Avg Reward â‰¥ 200?<br/>(Over last 25 episodes)"}}:::decision

    CONVERGED(["**Policy Converged**<br/>Save optimal weights<br/>Agent lands successfully"]):::result

    END([" Training Complete"]):::startend

    START --> RESET
    RESET --> EPISODE_LOOP
    EPISODE_LOOP -->|"Yes"| STEP_LOOP
    EPISODE_LOOP -->|"No"| CONVERGED
    STEP_LOOP -->|"Yes"| AGENT
    STEP_LOOP -->|"No"| EPSILON
    AGENT --> ENV
    ENV --> STORE
    STORE --> UPDATE_CHECK
    UPDATE_CHECK -->|"Yes"| SAMPLE
    UPDATE_CHECK -->|"No"| STEP_LOOP
    SAMPLE --> ONLINE
    ONLINE --> TARGET
    TARGET --> BELLMAN
    BELLMAN --> SYNC_CHECK
    SYNC_CHECK -->|"Yes"| SYNC
    SYNC_CHECK -->|"No"| STEP_LOOP
    SYNC --> STEP_LOOP
    EPSILON --> REWARD_CHECK
    REWARD_CHECK -->|"No"| EPISODE_LOOP
    REWARD_CHECK -->|"Yes"| CONVERGED
    CONVERGED --> END

    classDef startend fill:#1a7a1a,stroke:#4caf50,stroke-width:2px,color:#ffffff,font-weight:bold
    classDef database fill:#7b2d8b,stroke:#ce93d8,stroke-width:2px,color:#ffffff,font-weight:bold
    classDef mlmodel fill:#c62828,stroke:#ef9a9a,stroke-width:2px,color:#ffffff,font-weight:bold
    classDef process fill:#1565c0,stroke:#90caf9,stroke-width:2px,color:#ffffff,font-weight:bold
    classDef decision fill:#e65100,stroke:#ffcc02,stroke-width:3px,color:#ffffff,font-weight:bold
    classDef result fill:#2e7d32,stroke:#a5d6a7,stroke-width:2px,color:#ffffff,font-weight:bold
    classDef env fill:#4527a0,stroke:#ce93d8,stroke-width:2px,color:#ffffff,font-weight:bold

```

---

# âš™ï¸ Technologies Used

* Python
* PyTorch
* NumPy
* OpenAI Gym
* Matplotlib
* Pandas
* Jupyter Notebook

---

# ğŸ“‚ Project Structure

```text
Lunar-Landing-Program-Using-Reinforcement-Learning/
â”‚
â”œâ”€â”€ Images/
â”‚   â”œâ”€â”€ After_training.webp
â”‚   â”œâ”€â”€ before_training.webp
â”‚   â”œâ”€â”€ bellman adaptation equation.PNG
â”‚   â”œâ”€â”€ detach function.PNG
â”‚   â”œâ”€â”€ example of max and unsqueeze.PNG
â”‚   â”œâ”€â”€ exploration vs explotation.PNG
â”‚   â”œâ”€â”€ local vs target.PNG
â”‚   â”œâ”€â”€ Loss of Agent.PNG
â”‚   â”œâ”€â”€ loss function.PNG
â”‚   â”œâ”€â”€ next_Q_targate.PNG
â”‚   â”œâ”€â”€ Replaymemory.PNG
â”‚   â”œâ”€â”€ Soft Update.PNG
â”‚   â””â”€â”€ unsqueez.PNG
â”‚
â”œâ”€â”€ video_training/               
â”‚   â”œâ”€â”€ rl-video-episode-0.mp4
â”‚   â”œâ”€â”€ rl-video-episode-25.mp4
â”‚   â”œâ”€â”€ rl-video-episode-50.mp4
â”‚   â””â”€â”€ ... (other recorded episodes)
â”‚
â”œâ”€â”€ Lunar Landing Program Using RL.ipynb
â”œâ”€â”€ Lunar Landing Program Using RL.pdf
â”œâ”€â”€ Lunar_Landing_agent.pth
â”œâ”€â”€ Readme.md
â””â”€â”€ requirements.txt

```

---

# ğŸ”§ Hyperparameters

| Parameter | Value |
| --- | --- |
| Learning Rate | 0.0005 |
| Discount Factor (Î³) | 0.99 |
| Replay Buffer Size | 100000 |
| Batch Size | 150 |
| Episodes | 5000 |
| Max Steps | 1000 |
| Epsilon Start | 1.0 |
| Epsilon End | 0.01 |

---

# ğŸ“Š Training Process

Training follows these steps:

1. Reset the environment
2. Observe the current state
3. Select action using epsilon-greedy policy
4. Perform action
5. Receive reward and next state
6. Store experience in replay memory
7. Sample mini-batch
8. Update Q-network

Over time, the agent learns to perform **stable lunar landings**.

---

# ğŸ“ˆ Results

During training:

* Episode rewards increase gradually
* Landing becomes more stable
* The agent learns efficient control strategies

Performance is evaluated using **episode scores and rolling average rewards**.

## Agent Training Progress

# â–¶ï¸ How to Run the Project

### 1 Clone the repository

```bash
git clone https://github.com/udityamerit/Lunar-Landing-Program-Using-Reinforcement-Learning.git

```

### 2 Install dependencies

```bash
pip install torch gym numpy matplotlib pandas

```

### 3 Run the notebook

Open the notebook:

```bash
Lunar Landing Program Using RL.ipynb

```

Run all cells to start training the agent.

---

# ğŸ¯ Applications

This project demonstrates reinforcement learning applications in:

* Autonomous spacecraft landing
* Robotics control
* Autonomous vehicles
* Game AI
* Decision-making systems

---

# ğŸ‘¨â€ğŸ’» Author

**Uditya Narayan Tiwari**
B.Tech Computer Science Engineering (AI & ML)
VIT Bhopal University

ğŸŒ Portfolio
[https://udityanarayantiwari.netlify.app/](https://udityanarayantiwari.netlify.app/)

ğŸ’» GitHub
[https://github.com/udityamerit](https://github.com/udityamerit)

ğŸ“š Knowledge Base
[https://udityaknowledgebase.netlify.app/](https://udityaknowledgebase.netlify.app/)

```

```